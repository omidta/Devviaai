# Impact Measurement Template

## Implementation Information

**Feature/Idea ID**: `[IDEA-XXX]`  
**Feature Name**: `[Name]`  
**Implementation PR**: `#[PR number]`  
**Release Version**: `[v1.x.0]`  
**Release Date**: `[YYYY-MM-DD]`  
**Measurement Period**: `[Start Date]` to `[End Date]`  
**Framework Used**: `[Design Thinking / Lean Startup / Agile]`

---

## Original Expectations

### Success Criteria (from original idea)

1. `[Original success criterion 1]`
2. `[Original success criterion 2]`
3. `[Original success criterion 3]`

### Predicted Impact (from original idea)

| Metric | Baseline | Target | Source |
|--------|----------|--------|--------|
| [Metric 1] | [Value] | [Value] | Issue #[X] |
| [Metric 2] | [Value] | [Value] | Issue #[X] |
| [Metric 3] | [Value] | [Value] | Issue #[X] |

---

## Actual Results

### Quantitative Metrics

| Metric | Baseline | Target | Actual | Variance | Status |
|--------|----------|--------|--------|----------|--------|
| [Metric 1] | [Value] | [Value] | [Value] | [±X%] | ✅/❌ |
| [Metric 2] | [Value] | [Value] | [Value] | [±X%] | ✅/❌ |
| [Metric 3] | [Value] | [Value] | [Value] | [±X%] | ✅/❌ |

**Overall Target Achievement**: `[X of Y metrics met]` (`[X%]`)

### Data Collection Method

```
[Describe how data was collected]
- Tool/method 1: [Description]
- Tool/method 2: [Description]
- Sample size: [N users/events/etc.]
- Data period: [Date range]
```

### Statistical Significance

- **Sample Size**: `[N]`
- **Confidence Level**: `[X%]`
- **Margin of Error**: `[±X%]`
- **Statistically Significant?**: `[Yes / No / Inconclusive]`

---

## User Impact Assessment

### Adoption Metrics

| Metric | Target | Actual | Notes |
|--------|--------|--------|-------|
| Total users exposed | [N] | [N] | [Notes] |
| Active users (adopted) | [N] | [N] | [Notes] |
| Adoption rate | [X%] | [X%] | [Notes] |
| Time to first use (avg) | [X days] | [X days] | [Notes] |

### User Satisfaction

**Feedback Collection Method**: `[Survey / Interviews / Analytics / etc.]`

**Satisfaction Scores**:
| Question | Scale | Target | Actual | Met? |
|----------|-------|--------|--------|------|
| "This feature is useful" | 1-5 | > 4.0 | [X.X] | ✅/❌ |
| "It's easy to use" | 1-5 | > 4.0 | [X.X] | ✅/❌ |
| "It solves my problem" | 1-5 | > 4.0 | [X.X] | ✅/❌ |
| "I would recommend it" | 1-5 | > 4.0 | [X.X] | ✅/❌ |

**Net Promoter Score (NPS)**:
- Promoters (9-10): `[X%]`
- Passives (7-8): `[X%]`
- Detractors (0-6): `[X%]`
- **NPS Score**: `[X]` (Target: `[Y]`)

### Qualitative Feedback

**Positive Feedback** (themes):
1. `[Theme 1]`: "[Example quote]"
2. `[Theme 2]`: "[Example quote]"
3. `[Theme 3]`: "[Example quote]"

**Negative Feedback** (themes):
1. `[Theme 1]`: "[Example quote]"
2. `[Theme 2]`: "[Example quote]"
3. `[Theme 3]`: "[Example quote]"

**Suggestions for Improvement**:
1. `[Suggestion 1]`
2. `[Suggestion 2]`
3. `[Suggestion 3]`

---

## Business Impact

### Time & Cost Savings

| Impact Area | Before | After | Savings | Calculation |
|-------------|--------|-------|---------|-------------|
| Time per task | [X min] | [Y min] | [Z min] | [Method] |
| Tasks per day | [N] | [N] | - | - |
| **Daily time savings** | - | - | [X hours] | Z × N |
| Users affected | [N] | [N] | - | - |
| **Total daily savings** | - | - | [X hours] | Daily × Users |
| **Monthly savings** | - | - | [X hours] | Daily × 20 days |
| **Annual savings** | - | - | [X hours] | Monthly × 12 |
| **$ Value (if applicable)** | - | - | $[X] | Hours × Rate |

### Quality Improvements

| Quality Metric | Before | After | Improvement |
|----------------|--------|-------|-------------|
| Error rate | [X%] | [Y%] | [↓Z%] |
| Defect density | [X/1000] | [Y/1000] | [↓Z/1000] |
| Rework time | [X hours] | [Y hours] | [↓Z hours] |
| Customer satisfaction | [X/10] | [Y/10] | [↑Z points] |

### Productivity Gains

| Productivity Metric | Before | After | Change |
|---------------------|--------|-------|--------|
| Throughput | [N tasks/day] | [M tasks/day] | [↑X%] |
| Cycle time | [X days] | [Y days] | [↓Z%] |
| Resource utilization | [X%] | [Y%] | [±Z%] |

---

## Technical Performance

### Performance Metrics

| Metric | Target | Actual | Status |
|--------|--------|--------|--------|
| Response time | < [X ms] | [Y ms] | ✅/❌ |
| Throughput | > [X req/s] | [Y req/s] | ✅/❌ |
| Uptime | > [X%] | [Y%] | ✅/❌ |
| Error rate | < [X%] | [Y%] | ✅/❌ |

### Resource Utilization

| Resource | Budgeted | Actual | Variance |
|----------|----------|--------|----------|
| CPU | [X%] | [Y%] | [±Z%] |
| Memory | [X GB] | [Y GB] | [±Z GB] |
| Storage | [X GB] | [Y GB] | [±Z GB] |
| Network | [X Mbps] | [Y Mbps] | [±Z Mbps] |
| Cost | $[X]/month | $[Y]/month | $[±Z]/month |

---

## Lean Startup Validation (if applicable)

### Hypothesis Testing

**Original Hypothesis**:
> [Restate hypothesis from prototype]

**Validation Result**: `[Validated / Invalidated / Partially Validated]`

**Evidence**:
```
[Describe evidence that validates or invalidates the hypothesis]
- Evidence point 1
- Evidence point 2
- Evidence point 3
```

### Build-Measure-Learn Cycle

**Build Phase**:
- What was built: `[MVP description]`
- Time to build: `[X weeks]` (Planned: `[Y weeks]`)
- Cost: `$[X]` (Budgeted: `$[Y]`)

**Measure Phase**:
- Metrics tracked: `[List metrics]`
- Data collected over: `[X weeks]`
- Sample size: `[N users/events]`

**Learn Phase**:
- Key learnings:
  1. `[Learning 1]`
  2. `[Learning 2]`
  3. `[Learning 3]`

### Decision

- [ ] **Persevere**: Continue with current approach
- [ ] **Pivot**: Change direction based on learnings
- [ ] **Iterate**: Make improvements and re-measure
- [ ] **Scale**: Expand to more users/use cases
- [ ] **Sunset**: Discontinue based on poor results

**Reasoning**:
```
[Explain the decision based on data and learnings]
```

---

## Design Thinking Validation (if applicable)

### User Testing Results

**Test Participants**:
- Target persona: `[Persona name]`
- Number of testers: `[N]`
- Testing period: `[Date range]`

**Test Scenarios**:
1. Scenario 1: `[Description]`
   - Success rate: `[X%]` (Target: `[Y%]`)
   - Avg completion time: `[X min]` (Target: `[Y min]`)
   
2. Scenario 2: `[Description]`
   - Success rate: `[X%]` (Target: `[Y%]`)
   - Avg completion time: `[X min]` (Target: `[Y min]`)

### Iteration Feedback

**Changes Made Post-Launch**:
- Change 1: `[Description]` - Reason: `[User feedback]`
- Change 2: `[Description]` - Reason: `[User feedback]`
- Change 3: `[Description]` - Reason: `[User feedback]`

**User Empathy Validation**:
- [ ] Solution addresses the real user need
- [ ] Users find it intuitive and easy to use
- [ ] Reduces the identified pain points
- [ ] Users would recommend to others

---

## Agile Validation (if applicable)

### Acceptance Criteria Review

| Acceptance Criterion | Met? | Evidence |
|---------------------|------|----------|
| [Criterion 1] | ✅/❌ | [Evidence] |
| [Criterion 2] | ✅/❌ | [Evidence] |
| [Criterion 3] | ✅/❌ | [Evidence] |

**Acceptance Criteria Met**: `[X of Y]` (`[X%]`)

### Definition of Done Review

- [ ] All code merged and deployed
- [ ] All tests passing
- [ ] Documentation updated
- [ ] User acceptance testing passed
- [ ] Performance requirements met
- [ ] Security review completed
- [ ] Monitoring in place

**Definition of Done Status**: `[Complete / Partial / Not Met]`

---

## Unexpected Outcomes

### Positive Surprises

1. `[Unexpected benefit 1]`
   - Impact: `[Description]`
   - Evidence: `[Data or feedback]`

2. `[Unexpected benefit 2]`
   - Impact: `[Description]`
   - Evidence: `[Data or feedback]`

### Negative Surprises

1. `[Unexpected issue 1]`
   - Impact: `[Description]`
   - Mitigation: `[What was done]`

2. `[Unexpected issue 2]`
   - Impact: `[Description]`
   - Mitigation: `[What was done]`

---

## Lessons Learned

### What Went Well

1. `[Success 1]`
2. `[Success 2]`
3. `[Success 3]`

### What Could Be Improved

1. `[Improvement area 1]`
2. `[Improvement area 2]`
3. `[Improvement area 3]`

### Recommendations for Future Ideas

1. `[Recommendation 1]`
2. `[Recommendation 2]`
3. `[Recommendation 3]`

---

## Next Steps

### Immediate Actions

- [ ] Action 1: `[Description]` - Owner: `[@username]` - Due: `[Date]`
- [ ] Action 2: `[Description]` - Owner: `[@username]` - Due: `[Date]`
- [ ] Action 3: `[Description]` - Owner: `[@username]` - Due: `[Date]`

### Future Enhancements

Based on this measurement, future enhancements to consider:

1. **Enhancement 1**: `[Description]`
   - **Rationale**: `[Why]`
   - **Priority**: `[High / Medium / Low]`

2. **Enhancement 2**: `[Description]`
   - **Rationale**: `[Why]`
   - **Priority**: `[High / Medium / Low]`

---

## Final Assessment

### Overall Success Rating

**Success Score**: `[X/10]`

**Justification**:
```
[Explain the overall success rating based on:
- Metrics achievement
- User satisfaction
- Business impact
- Technical performance
- Unexpected outcomes]
```

### Return on Investment (ROI)

**Investment**:
- Development cost: `$[X]` or `[Y person-weeks]`
- Infrastructure cost: `$[X]/month`
- Total first-year cost: `$[X]`

**Return**:
- Annual time savings value: `$[X]`
- Quality improvement value: `$[X]`
- Other quantified benefits: `$[X]`
- **Total first-year value**: `$[X]`

**ROI**: `[X%]` or `[X:1 ratio]`

### Recommendation

- [ ] **Success - Continue & Enhance**: Feature is successful, invest in improvements
- [ ] **Success - Maintain**: Feature is successful, keep as-is
- [ ] **Partial Success - Iterate**: Results are mixed, make improvements and re-measure
- [ ] **Failure - Sunset**: Feature failed to meet objectives, plan deprecation

---

## Sign-off

**Measurement Completed By**: `[@username]` - `[Role]`  
**Measurement Date**: `[YYYY-MM-DD]`  
**Reviewed By**: `[@username]` - `[Role]`  
**Review Date**: `[YYYY-MM-DD]`

---

## Appendix

### Data Sources

- Source 1: `[Description and link]`
- Source 2: `[Description and link]`
- Source 3: `[Description and link]`

### Supporting Documents

- Document 1: `[Link]`
- Document 2: `[Link]`
- Dashboard: `[Link]`

### Raw Data

```
[Include or link to raw data used for analysis]
```

---

**Template Version**: 1.0.0  
**Last Updated**: 2026-02-01
